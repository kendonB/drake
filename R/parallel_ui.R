#' @title Function \code{batchtools_drake_tmpl_file}
#' @description Write the batchtools template file
#' from one of the examples. If there are multiple files,
#' only the first one (alphabetically) is written.
#' @export
#' @seealso \code{\link{drake_examples}}, \code{\link{drake_example}},
#' \code{\link{shell_file}}
#' @return \code{NULL} is returned,
#' but a batchtools template file is written.
#' @param example Name of the drake example
#' from which to take the template file.
#' Must be listed in \code{\link{drake_examples}()}.
#' @param to Character vector, where to write the file.
#' @param overwrite Logical, whether to overwrite an existing file of the
#' same name.
#' @examples
#' \dontrun{
#' load_basic_example() # Load the canonical example.
#' # List the drake examples. Only some have template files.
#' drake_examples()
#' # Write the batchtools template file from the SLURM example.
#' batchtools_drake_tmpl_file("slurm") # Writes batchtools.slurm.tmpl.
#' # Find batchtools.slurm.tmpl with the rest of the example's files.
#' drake_example("slurm") # Writes a new 'slurm' folder with more files.
#' # Run the basic example with a
#' # SLURM-powered parallel backend. Requires SLURM.
#' library(future.batchtools)
#' future::plan(batchtools_slurm(template = "batchtools.slurm.tmpl"))
#' make(my_plan, parallelism = "future_lapply")
#' }
batchtools_drake_tmpl_file <- function(
  example = drake::drake_examples(),
  to = getwd(),
  overwrite = FALSE
){
  example <- match.arg(example)
  dir <- system.file(file.path("examples", example), package = "drake",
    mustWork = TRUE)
  files <- list.files(dir)
  template_files <- files[grepl("\\.tmpl$", files)]
  if (!length(template_files)){
    stop("No template files found for the ", example, " example.")
  }
  file <- file.path(dir, template_files[1])
  file.copy(from = file, to = to,
    overwrite = overwrite, recursive = TRUE)
  invisible()
}

#' @title Function \code{parallelism_choices}
#' @description List the types of supported parallel computing.
#' @export
#' @seealso \code{\link{make}}, \code{\link{shell_file}}
#' @return Character vector listing the types of parallel
#' computing supported.
#'
#' @details Run \code{make(..., parallelism = x, jobs = n)} for any of
#' the following values of \code{x} to distribute targets over parallel
#' units of execution.
#' \describe{
#'  \item{'parLapply'}{launches multiple processes in a single R session
#'  using \code{parallel::\link{parLapply}()}.
#'  This is single-node, (potentially) multicore computing.
#'  It requires more overhead than the \code{'mclapply'} option,
#'  but it works on Windows. If \code{jobs} is \code{1} in
#'  \code{\link{make}()}, then no 'cluster' is created and
#'  no parallelism is used.}
#'
#'  \item{'mclapply'}{uses multiple processes in a single R session.
#'  This is single-node, (potentially) multicore computing.
#'  Does not work on Windows for \code{jobs > 1}
#'  because \code{\link{mclapply}()} is based on forking.}
#'
#'  \item{'future_lapply'}{
#'  opens up a whole trove of parallel backends
#'  powered by the \code{future} and \code{future.batchtools}
#'  packages. First, set the parallel backend globally using
#'  \code{future::plan()}.
#'  Then, apply the backend to your workplan
#'  using \code{make(..., parallelism = "future_lapply", jobs = ...)}.
#'  But be warned: the environment for each target needs to be set up
#'  from scratch, so this backend type is higher overhead than either
#'  \code{mclapply} or \code{parLapply}.
#'  Also, the \code{jobs} argument only applies to the imports.
#'  To set the max number of jobs, set the \code{workers}
#'  argument where it exists. For example, call
#'  \code{future::plan(multisession(workers = 4))},
#'  then call \code{\link{make}(your_plan, parallelism = "future_lapply")}.
#'  You might also try options(mc.cores = jobs),
#'  or see \code{?future::future::.options}
#'  for environment variables that set the max number of jobs.
#'  }
#'
#'  \item{'Makefile'}{uses multiple R sessions
#'  by creating and running a Makefile.
#'  For distributed computing on a cluster or supercomputer,
#'  try \code{\link{make}(..., parallelism = 'Makefile',
#'  prepend = 'SHELL=./shell.sh')}.
#'  You need an auxiliary \code{shell.sh} file for this,
#'  and \code{\link{shell_file}()}
#'  writes an example.
#'
#'  Here, Makefile-level parallelism is only used for
#'  targets in your workflow plan
#'  data frame, not imports. To process imported objects and files,
#'  drake selects the best parallel
#'  backend for your system and uses
#'  the number of jobs you give to the \code{jobs}
#'  argument to \code{\link{make}()}.
#'  To use at most 2 jobs for imports and at most 4 jobs
#'  for targets, run
#'  \code{make(..., parallelism = 'Makefile', jobs = 2, args = '--jobs=4')}
#'
#'  Caution: the Makefile generated by
#'  \code{\link{make}(..., parallelism = 'Makefile')}
#'  is NOT standalone. DO NOT run it outside of
#' \code{\link{make}()} or \code{\link{make}()}.
#'  Also, Windows users will need to download and install Rtools.
#' }}
#'
#' @param distributed_only logical, whether to return only
#' the distributed backend types, such as \code{Makefile} and
#' \code{parLapply}
#'
#' @examples
#' # See all the parallel computing options.
#' parallelism_choices()
#' # See just the distributed computing options.
#' parallelism_choices(distributed_only = TRUE)
parallelism_choices <- function(distributed_only = FALSE) {
  local <- c(
    "parLapply",
    "mclapply"
  )
  distributed <- c(
    "Makefile",
    "future_lapply"
  )
  if (distributed_only){
    distributed
  } else {
    c(local, distributed)
  }
}

#' @title Function \code{default_parallelism}
#' @description Show the default parallelism for \code{\link{make}()}
#' on your system:
#' \code{'parLapply'} for Windows machines and \code{'mclapply'}
#' for other platforms.
#' @export
#' @seealso \code{\link{make}}, \code{\link{shell_file}}
#' @return The default parallelism option for your system.
#' @examples
#' default_parallelism()
default_parallelism <- function() {
  ifelse(on_windows(), "parLapply", "mclapply") %>%
    unname
}

#' @title Function \code{max_useful_jobs}
#' @description Get the maximum number of useful jobs in the next call
#' to \code{make(..., jobs = YOUR_CHOICE)}.
#' @details Any additional jobs more than \code{max_useful_jobs(...)}
#' will be superfluous, and could even slow you down for
#' \code{make(..., parallelism = 'parLapply')}. Set
#' Set the \code{imports} argument to change your assumptions about
#' how fast objects/files are imported.
#' IMPORTANT: you must be in the root directory of your project.
#' @export
#'
#' @return A numeric scalar, the maximum number of useful jobs for
#' \code{\link{make}(..., jobs = ...)}.
#'
#' @seealso \code{\link{vis_drake_graph}}, \code{\link{build_drake_graph}},
#' \code{\link{shell_file}}
#'
#' @param plan workflow plan data frame, same as for function
#' \code{\link{make}()}.
#'
#' @param from_scratch logical, whether to compute the max
#' useful jobs as if the workplan were to run from scratch
#' (with all targets out of date).
#'
#' @param targets names of targets to build, same as for function
#' \code{\link{make}()}.
#'
#' @param envir environment to import from, same as for function
#' \code{\link{make}()}. \code{config$envir} is ignored in favor of
#' \code{envir}.
#'
#' @param verbose logical, whether to output messages to the console.
#'
#' @param hook same as for \code{\link{make}}
#'
#' @param cache optional drake cache. See code{\link{new_cache}()}. If
#' The \code{cache} argument is ignored if a non-null
#' \code{config} argument is supplied.
#'
#' @param jobs The \code{outdated()} function is called internally,
#' and it needs to import objects and examine your
#' input files to see what has been updated. This could take some time,
#' and parallel computing may be needed
#' to speed up the process. The \code{jobs} argument is number of parallel jobs
#' to use for faster computation.
#'
#' @param parallelism Choice of parallel backend to speed up the computation.
#' Execution order in \code{\link{make}()} is slightly different
#' when \code{parallelism} equals \code{'Makefile'}
#' because in that case, all the imports are
#' imported before any target is built.
#' Thus, \code{max_useful_jobs()} may give a
#' different answer for Makefile parallelism.
#' See \code{?parallelism_choices} for details.
#'
#' @param packages same as for \code{\link{make}}
#'
#' @param prework same as for \code{\link{make}}
#'
#' @param config internal configuration list of \code{\link{make}(...)},
#' produced also with \code{\link{config}()}.
#' \code{config$envir} is ignored.
#' Otherwise, if not \code{NULL}, \code{config}
#' overrides all the other arguments except
#' \code{imports} and \code{from_scratch}.
#' For example,
#' \code{plan} is replaced with \code{config$plan}.
#' Computing \code{\link{config}}
#' in advance could save time if you plan multiple calls to
#' \code{dataframes_graph()}.
#'
#' @param imports Set the \code{imports} argument to change your
#' assumptions about how fast objects/files are imported.
#' Possible values:
#' \itemize{
#'  \item{'all'}{: Factor all imported files/objects into
#'    calculating the max useful number of jobs.
#'    Note: this is not appropriate for
#'    \code{make(.., parallelism = 'Makefile')} because imports
#'    are processed sequentially for the Makefile option.}
#'  \item{'files'}{: Factor all imported files into the calculation,
#'    but ignore all the other imports.}
#'  \item{'none'}{: Ignore all the imports and just focus on the max number
#'    of useful jobs for parallelizing targets.}
#' }
#'
#' @param make_imports logical, whether to import external files
#' and objects from the user's workspace to determine
#' which targets are up to date. If \code{FALSE}, the computation
#' is faster, but all the relevant information is drawn from the cache
#' and may be out of date.
#'
#' @examples
#' \dontrun{
#' load_basic_example() # Load drake's canonical example.
#' # Look at the graph. The work proceeds column by column
#' # in parallelizable stages. The maximum number of useful jobs
#' # is determined by the number and kind of targets/imports
#' # in the columns.
#' vis_drake_graph(my_plan)
#' # Should be 8 because everythign is out of date.
#' max_useful_jobs(my_plan) # 8
#' # Take into account targets and imported files.
#' max_useful_jobs(my_plan, imports = 'files') # 8
#' # Include imported R objects too.
#' max_useful_jobs(my_plan, imports = 'all') # 10
#' # Exclude all imported objects.
#' max_useful_jobs(my_plan, imports = 'none') # 8
#' make(my_plan) # Run the project, build the targets.
#' vis_drake_graph(my_plan) # Everything is up to date.
#' # Ignore the targets already built.
#' max_useful_jobs(my_plan) # 1
#' max_useful_jobs(my_plan, imports = 'files') # 1
#' # Imports are never really skipped in make().
#' max_useful_jobs(my_plan, imports = 'all') # 9
#' max_useful_jobs(my_plan, imports = 'none') # 0
#' # Change a function so some targets are now out of date.
#' reg2 = function(d){
#'   d$x3 = d$x^3
#'   lm(y ~ x3, data = d)
#' }
#' vis_drake_graph(my_plan)
#' # We have different numbers of max useful jobs.
#' # By default, the output takes into account which
#' # targets are out of date. To disable, consider
#' # using the from_scratch argument.
#' max_useful_jobs(my_plan) # 4
#' max_useful_jobs(my_plan, from_scratch = TRUE) # 8
#' max_useful_jobs(my_plan, imports = 'files') # 4
#' max_useful_jobs(my_plan, imports = 'all') # 9
#' max_useful_jobs(my_plan, imports = 'none') # 4
#' }
max_useful_jobs <- function(
  plan = workplan(), from_scratch = FALSE,
  targets = drake::possible_targets(plan),
  envir = parent.frame(), verbose = TRUE,
  hook = default_hook,
  cache = drake::get_cache(verbose = verbose),
  jobs = 1, parallelism = drake::default_parallelism(),
  packages = rev(.packages()), prework = character(0), config = NULL,
  imports = c("files", "all", "none"),
  make_imports = TRUE
){
  force(envir)
  if (is.null(config)){
    config <- drake_config(
      plan = plan,
      targets = targets,
      envir = envir,
      verbose = verbose,
      hook = hook,
      cache = cache,
      parallelism = parallelism,
      jobs = jobs,
      packages = packages,
      prework = prework
    )
  }
  nodes <- dataframes_graph(plan = config$plan, config = config,
    split_columns = FALSE, make_imports = make_imports,
    from_scratch = from_scratch)$nodes
  imports <- match.arg(imports)
  just_targets <- intersect(nodes$id, config$plan$target)
  just_files <- Filter(x = nodes$id, f = is_file)
  targets_and_files <- union(just_targets, just_files)
  if (imports == "none"){
    nodes <- nodes[just_targets, ]
  } else if (imports == "files"){
    nodes <- nodes[targets_and_files, ]
  }
  if (!from_scratch){
    nodes <- nodes[nodes$status != "up to date", ]
  }
  if (!nrow(nodes))
    return(0)
  dlply(nodes, "level", nrow) %>%
    unlist %>%
    max
}

#' @title Function shell_file
#' @description Write an example \code{shell.sh} file required by
#' \code{make(..., parallelism = 'Makefile', prepend = 'SHELL=./shell.sh')}
#' and do a `chmod +x` to enable execution.
#' Use this option to run your project in parallel on a computing cluster
#' or supercomputer.
#' @seealso \code{\link{make}}, \code{\link{max_useful_jobs}},
#' \code{\link{parallelism_choices}}, \code{\link{batchtools_drake_tmpl_file}},
#' \code{\link{drake_example}}, \code{\link{drake_examples}}
#' @export
#' @return The return value of the call to \code{file.copy()} that
#' wrote the shell file.
#' @param path file path of the shell file
#' @param overwrite logical, whether to overwrite a possible
#' destination file with the same name
#' @examples
#' \dontrun{
#' # Write shell.sh to your working directory.
#' # Read the parallelism vignette to learn how it is used
#' # in Makefile parallelism.
#' shell_file()
#' }
shell_file <- function(
  path = "shell.sh",
  overwrite = FALSE
){
  from <- system.file("shell.sh", package = "drake", mustWork = TRUE)
  if (file.exists(path) & overwrite){
    warning("Overwriting file ", path)
  }
  invisible(file.copy(from = from, to = path, copy.mode = TRUE,
    overwrite = overwrite))
}
